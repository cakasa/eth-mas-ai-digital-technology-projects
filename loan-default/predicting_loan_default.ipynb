{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c49bdf-1cec-433f-aa56-a73b63689add",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77a33b40-7e0a-4314-9983-4f7383d2dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in /cluster/home/bandonov/.local/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/site-packages (from imbalanced-learn) (1.5.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /cluster/home/bandonov/.local/lib/python3.11/site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/site-packages (from imbalanced-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install imblearn to deal with class imbalance\n",
    "import sys\n",
    "!{sys.executable} -m pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5ed8d71-a1de-4930-96f6-00b033038b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTENC, ADASYN\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc9bd5-2adb-470c-a0af-61dce8b3b609",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d8ecd37-8650-42e1-bbe5-0265da01fd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.20402340.bandonov/ipykernel_1405733/2434044024.py:1: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  loans = pd.read_csv('./accepted_2007_to_2018Q4.csv')\n"
     ]
    }
   ],
   "source": [
    "loans = pd.read_csv('./accepted_2007_to_2018Q4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3443dcd-9144-4ee9-9cfe-00e58154e9c2",
   "metadata": {},
   "source": [
    "#### Removal of unavailable and unusable features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea515626-3743-406f-a85d-dd863b6a5398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unavailable features: 42 / 151\n"
     ]
    }
   ],
   "source": [
    "# These are features that are unavailable at inference time, and cannot be used.\n",
    "unavailable_features = [\n",
    "    # 'chargeoff_within_12_mths',\n",
    "    'collection_recovery_fee',\n",
    "    # 'dti',\n",
    "    # 'dti_joint',\n",
    "    'funded_amnt',\n",
    "    'funded_amnt_inv',\n",
    "    'initial_list_status',\n",
    "    'issue_d',\n",
    "    'last_credit_pull_d',\n",
    "    'last_fico_range_high',\n",
    "    'last_fico_range_low',\n",
    "    'last_pymnt_amnt',\n",
    "    'last_pymnt_d',\n",
    "    'next_pymnt_d',\n",
    "    'out_prncp',\n",
    "    'out_prncp_inv',\n",
    "    'pymnt_plan',\n",
    "    'recoveries',\n",
    "    'total_pymnt',\n",
    "    'total_pymnt_inv',\n",
    "    'total_rec_int',\n",
    "    'total_rec_late_fee',\n",
    "    'total_rec_prncp',\n",
    "    'hardship_flag',\n",
    "    'hardship_type',\n",
    "    'hardship_reason',\n",
    "    'hardship_status',\n",
    "    'deferral_term',\n",
    "    'hardship_amount',\n",
    "    'hardship_start_date',\n",
    "    'hardship_end_date',\n",
    "    'payment_plan_start_date',\n",
    "    'hardship_length',\n",
    "    'hardship_dpd',\n",
    "    'hardship_loan_status',\n",
    "    'orig_projected_additional_accrued_interest',\n",
    "    'hardship_payoff_balance_amount',\n",
    "    'hardship_last_payment_amount',\n",
    "    'debt_settlement_flag',\n",
    "    'debt_settlement_flag_date',\n",
    "    'settlement_status',\n",
    "    'settlement_date',\n",
    "    'settlement_amount',\n",
    "    'settlement_percentage',\n",
    "    'settlement_term',\n",
    "]\n",
    "\n",
    "print(f'Unavailable features: {len(unavailable_features)} / {len(loans.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f25667e-7537-4977-a75b-d73fc40d90b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unused features: 11 / 151\n"
     ]
    }
   ],
   "source": [
    "# These are features which are available at runtime, but should not be used for inference, either because they introduce bias,\n",
    "# or because they first have to be converted into a meaningful representation before being able to be used (e.g. loan description)\n",
    "unused_features = [\n",
    "    'addr_state',\n",
    "    'desc',\n",
    "    'grade',\n",
    "    'emp_title',\n",
    "    'id',\n",
    "    'member_id',\n",
    "    #    'open_act_il',\n",
    "    'policy_code',\n",
    "    'sub_grade',\n",
    "    'title',\n",
    "    #    'total_bal_il',\n",
    "    'url',\n",
    "    'zip_code',\n",
    "    #    'sec_app_open_act_il'\n",
    "]\n",
    "\n",
    "# TODO: experiment with addr_state and zip_code, leave out (sub_)grade.\n",
    "print(f'Unused features: {len(unused_features)} / {len(loans.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b524dce-26d5-4c9f-bd2f-9f52ac08b28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining features: 98\n"
     ]
    }
   ],
   "source": [
    "# Drop these two sets of features\n",
    "features_to_drop = unavailable_features + unused_features\n",
    "loans = loans.drop(columns=features_to_drop)\n",
    "print(f'Remaining features: {len(loans.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "deb6bd4d-403a-4a84-9ecf-2fbb622da312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the loan defaults and the fully paid loans\n",
    "defaulted_loans_labels = ['Charged Off', 'Default', 'Does not meet the credit policy. Status:Charged Off']\n",
    "default_loans = loans[loans['loan_status'].isin(defaulted_loans_labels)]\n",
    "paid_loans = loans[loans['loan_status'] == 'Fully Paid']\n",
    "\n",
    "# Join the loan defaults and paid loans. This is now our dataset.\n",
    "loans = pd.concat([paid_loans, default_loans])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc00a5a-6682-48de-9c13-e43fb347ca61",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dea33aba-5399-4a53-9433-f3cf254ed814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan defaults: 269360\n",
      "Fully paid loans: 1076751\n",
      "There is data imbalance!\n"
     ]
    }
   ],
   "source": [
    "# See how loan labels are distributed ('loan_status' is the variable we would like to predict in this task)\n",
    "status_counts = Counter(loans['loan_status'])\n",
    "n_loan_defaults = sum([status_counts[label] for label in defaulted_loans_labels])\n",
    "n_paid_loans = status_counts['Fully Paid']\n",
    "print(f\"Loan defaults: {n_loan_defaults}\")\n",
    "print(f\"Fully paid loans: {n_paid_loans}\")\n",
    "print(\"There is data imbalance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a4cb29e-becf-4ea0-8ef0-e8a19b42edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual loans: 1320305 , 0.98\n",
      "Joint loans: 25806 , 0.02\n"
     ]
    }
   ],
   "source": [
    "individual_loans = loans[loans['application_type'] == 'Individual']\n",
    "print(\"Individual loans:\", len(individual_loans), \",\", round(len(individual_loans) / len(loans), 2))\n",
    "print(\"Joint loans:\", len(loans) - len(individual_loans), \",\", round((len(loans) - len(individual_loans)) / len(loans), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5717ea33-245e-40ef-be92-f61c17b2c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features: 87\n",
      "Categorical features: 11\n",
      "term -> [' 36 months', ' 60 months'], 2, False\n",
      "emp_length -> ['10+ years', '3 years', '4 years', '6 years', '8 years', '2 years', '9 years', '< 1 year', '1 year', '5 years'], 12, True\n",
      "home_ownership -> ['MORTGAGE', 'RENT', 'OWN', 'ANY', 'NONE', 'OTHER'], 6, False\n",
      "verification_status -> ['Not Verified', 'Source Verified', 'Verified'], 3, False\n",
      "loan_status -> ['Fully Paid', 'Charged Off', 'Default', 'Does not meet the credit policy. Status:Charged Off'], 4, False\n",
      "purpose -> ['debt_consolidation', 'small_business', 'home_improvement', 'major_purchase', 'credit_card', 'other', 'house', 'vacation', 'medical', 'car'], 14, False\n",
      "earliest_cr_line -> ['Aug-2003', 'Dec-1999', 'Aug-2000', 'Jun-1998', 'Oct-1987', 'Jun-1990', 'Feb-1999', 'Apr-2002', 'Nov-1994', 'Jun-1996'], 740, True\n",
      "application_type -> ['Individual', 'Joint App'], 2, False\n",
      "verification_status_joint -> [nan, 'Not Verified', 'Verified', 'Source Verified'], 4, True\n",
      "sec_app_earliest_cr_line -> [nan, 'Feb-2007', 'Feb-2009', 'May-2004', 'Mar-2007', 'Jul-2015', 'Aug-2012', 'Jun-1999', 'Aug-2006', 'Sep-2005'], 571, True\n",
      "disbursement_method -> ['Cash', 'DirectPay'], 2, False\n",
      "Number of features with NA: 4\n"
     ]
    }
   ],
   "source": [
    "float_cols = loans.select_dtypes(include='float64')\n",
    "object_cols = loans.select_dtypes(include='object')\n",
    "\n",
    "print(f'Numerical features: {len(float_cols.columns)}')\n",
    "print(f'Categorical features: {len(object_cols.columns)}')\n",
    "\n",
    "possible_values = {}\n",
    "for col in object_cols.columns:\n",
    "    possible_values[col] = list(pd.unique(object_cols[col]))\n",
    "\n",
    "n_na = 0\n",
    "for key, val in possible_values.items():\n",
    "    has_na_vals = any(pd.isna(val))\n",
    "    print(f'{key} -> {val[:10]}, {len(val)}, {has_na_vals}')\n",
    "    if has_na_vals:\n",
    "        n_na += 1\n",
    "\n",
    "print(\"Number of features with NA:\", n_na) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb79e06-f21c-4a4e-8ab5-4b9444934366",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ddd11d-2393-4263-805a-5cdccfcb7b1d",
   "metadata": {},
   "source": [
    "#### Convert text features into numerical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3bee2f1-8edd-4309-b42e-21187c7dfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_employment_years(years_string):\n",
    "        if type(years_string) is not str:\n",
    "            return years_string\n",
    "    \n",
    "        if '<' in years_string:\n",
    "            return 0\n",
    "        \n",
    "        if '10' in years_string:\n",
    "            return 10\n",
    "        \n",
    "        return int(years_string[0])\n",
    "\n",
    "def convert_text_features_into_numerical_ones(loans_orig: pd.DataFrame, in_place=False):\n",
    "    loans = loans_orig if in_place else loans_orig.copy()\n",
    "\n",
    "    # Impute categorical features by using dummy variables\n",
    "    non_na_categorical_features = [\n",
    "        'term',\n",
    "        'home_ownership',\n",
    "        'verification_status',\n",
    "        'purpose',\n",
    "        'disbursement_method',\n",
    "        'application_type'\n",
    "    ]\n",
    "\n",
    "    na_categorical_features = ['verification_status_joint']\n",
    "    \n",
    "    dummies_for_non_na_categorical_features = pd.get_dummies(loans[non_na_categorical_features], dtype='float64', drop_first=True)\n",
    "    dummies_for_na_categorical_features = pd.get_dummies(loans[na_categorical_features], dtype='float64', drop_first=True, dummy_na=True)\n",
    "    \n",
    "    loans = pd.concat([\n",
    "        loans.drop(non_na_categorical_features + na_categorical_features, axis=1),\n",
    "        dummies_for_non_na_categorical_features,\n",
    "        dummies_for_na_categorical_features\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Convert dates of the format Mon-Year to just an integer representing the year\n",
    "    map_datestr_to_year = lambda x: int(x[-4:]) if type(x) is str else x\n",
    "    loans['earliest_cr_line'] = loans['earliest_cr_line'].map(map_datestr_to_year)\n",
    "    loans['sec_app_earliest_cr_line'] = loans['sec_app_earliest_cr_line'].map(map_datestr_to_year)\n",
    "\n",
    "    # Convert the emp_length string to a number representing the number of employment years.\n",
    "    loans['emp_length'] =  loans['emp_length'].map(get_employment_years)\n",
    "\n",
    "    # Convert the indicator variable into a numerical one. These are the 2 classes we want to predict\n",
    "    map_loan_status = lambda x: 1 if x == 'Fully Paid' else 0\n",
    "    loans['loan_status'] = loans['loan_status'].map(map_loan_status)\n",
    "\n",
    "    return loans\n",
    "\n",
    "loans = convert_text_features_into_numerical_ones(loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec48ce4-87fa-4e00-9bc6-bc454374e816",
   "metadata": {},
   "source": [
    "#### Feature imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c1c94-19d1-49c3-9df4-b63934595911",
   "metadata": {},
   "source": [
    "##### Approach 1: Simple imputation. Fast, but not as accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c0e9a7a-5272-4bfd-b86c-3648a9637e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def simple_imputer(df):\n",
    "    simple_imputer = SimpleImputer(strategy='median')\n",
    "    imputed_df = pd.DataFrame(simple_imputer.fit_transform(df), columns=df.columns)\n",
    "    return imputed_df, simple_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e944e-f533-4b1f-b415-2a40817d67d4",
   "metadata": {},
   "source": [
    "##### (WIP) Approach 2: More involved imputation by manually selecting how each feature should be imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "248cb916-332e-4772-88a5-73b75570aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def involved_imputer(df):\n",
    "    features = loans.columns\n",
    "    na_features = [feature for feature in features if any(pd.isna(loans[feature]))]\n",
    "    print(len(na_features), '/', len(features), na_features)\n",
    "    for feature in na_features[:10]:\n",
    "        to_plot = loans[feature].copy()\n",
    "        plt.hist(to_plot)\n",
    "        # plt.yscale('log')\n",
    "        plt.title(feature)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb5a71-3a32-4f2c-9094-56b7c483f370",
   "metadata": {},
   "source": [
    "#### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f80421c0-1d92-477f-8f14-6c2fe9836940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_scaler(df):\n",
    "    robust_scaler = RobustScaler()\n",
    "    scaled_df = pd.DataFrame(robust_scaler.fit_transform(df), columns=df.columns)\n",
    "    return scaled_df, robust_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafaf46-55b0-4dff-9f9d-159c2cc32fc1",
   "metadata": {},
   "source": [
    "#### Extract features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e9d100b-4813-4611-aac7-c00693f4fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalanced_X = loans.loc[:, loans.columns != 'loan_status']\n",
    "imbalanced_y = loans['loan_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f26951-b729-4466-be5f-b155bfef4235",
   "metadata": {},
   "source": [
    "#### Deal with class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0645cc87-1d03-4b06-9c47-b8e76cbeadf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables used in all of the following approaches\n",
    "class_sizes = [len(imbalanced_y[imbalanced_y == i])for i in range(2)]\n",
    "size_difference = abs(class_sizes[0] - class_sizes[1])\n",
    "minority_class = np.argmin(class_sizes)\n",
    "majority_class = 1 - minority_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a770393-d25f-48e3-92cd-0cac4179c193",
   "metadata": {},
   "source": [
    "##### Approach 1: Oversampling the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b824fe8-5331-48ed-9c96-d23a1706ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Duplicate examples from the minority class, until sizes of both classes match\n",
    "def duplicate_minority_class(X_imbalanced, y_imbalanced):\n",
    "    index_to_resample = y_imbalanced == minority_class\n",
    "    upsampled_minority_class = X_imbalanced[index_to_resample].sample(n=size_difference,  replace=True)\n",
    "    upsampled_minority_labels = y_imbalanced[upsampled_minority_class.index]\n",
    "    X = pd.concat([X_imbalanced, upsampled_minority_class])\n",
    "    y = pd.concat([y_imbalanced, upsampled_minority_labels])\n",
    "    \n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    reshuffled_index = np.random.permutation(X.index)\n",
    "    X = X.reindex(reshuffled_index)\n",
    "    y = y.reindex(reshuffled_index)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36b24d1b-21f4-4b3d-89a2-26bf872a3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 SMOTE (Create synthetic samples by interpolating between existing minority samples)\n",
    "# https://arxiv.org/pdf/1106.1813\n",
    "# Note: This might take several minutes.\n",
    "def generate_synthetic_datapoints(row_index, neighbor_indices, samples, n_samples):\n",
    "    s = time.time()\n",
    "    row = samples.iloc[row_index]\n",
    "    neighbor_indices = np.random.choice(neighbor_indices, n_samples)\n",
    "    neighbors = samples.iloc[neighbor_indices]\n",
    "\n",
    "    generated_samples = []\n",
    "    for index, neighbor in neighbors.iterrows():\n",
    "        alpha = random.random()\n",
    "        new_sample = (1 - alpha) * row + alpha * neighbor\n",
    "\n",
    "        categorical_choices = np.array([row[categorical_features], neighbor[categorical_features]])\n",
    "        categorical_indices = np.random.randint(0, 2, size=len(categorical_features))\n",
    "        new_sample[categorical_features] = categorical_choices[categorical_indices, np.arange(len(categorical_features))]\n",
    "        generated_samples.append(new_sample)\n",
    "        \n",
    "    e = time.time()\n",
    "    return generated_samples\n",
    "\n",
    "def smote(X_imbalanced, y_imbalanced, k=5):\n",
    "    index_to_resample = y_imbalanced == minority_class\n",
    "    minority_samples = X_imbalanced[index_to_resample]\n",
    "    \n",
    "    smote_n = int(np.ceil(size_difference / class_sizes[minority_class]))\n",
    "    synthetic_samples = []\n",
    "    nearest_neighbors = NearestNeighbors(n_neighbors=k)\n",
    "    nearest_neighbors.fit(minority_samples)\n",
    "\n",
    "    neighbor_indices_per_row = nearest_neighbors.kneighbors(minority_samples, n_neighbors=k+1, return_distance=False)\n",
    "\n",
    "    for neighbor_indices in neighbor_indices_per_row:\n",
    "        generated_datapoints = generate_synthetic_datapoints(neighbor_indices[0], neighbor_indices[1:], minority_samples, smote_n)\n",
    "        synthetic_samples.extend(generated_datapoints)\n",
    "\n",
    "    synthetic_labels = pd.Series([minority_class] * len(synthetic_samples))\n",
    "    y = pd.concat([y_imbalanced, synthetic_labels])\n",
    "    synthetic_samples_df = pd.DataFrame(synthetic_samples)\n",
    "    X = pd.concat([X_imbalanced, synthetic_samples_df])\n",
    "    \n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    reshuffled_index = np.random.permutation(X.index)\n",
    "    X = X.reindex(reshuffled_index)\n",
    "    y = y.reindex(reshuffled_index)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ea1ede6-04a6-418b-bf25-e54ef64ab783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# 1.3 ADASYN (Adaptive synthetic sampling)\n",
    "# https://ieeexplore.ieee.org/document/4633969\n",
    "def adasyn_auto(X_imbalanced, y_imbalanced, k=5):\n",
    "    smote = ADASYN(categorical_features=categorical_features, k_neighbors=nn)\n",
    "    X, y = smote.fit_resample(imbalanced_X, imbalanced_y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54806c41-b91f-4bed-b878-534a9a2e2d8e",
   "metadata": {},
   "source": [
    "##### Approach 2: Undersampling the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cff3edf-21bb-4f9b-adf4-4404b572e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Random undersampling (Select a random sample of the majority class, such that the sizes of both classes match)\n",
    "def random_undersampling(X_imbalanced, y_imbalanced):\n",
    "    index_to_resample = y_imbalanced == majority_class\n",
    "    undersampled_majority_class = X_imbalanced[index_to_resample].sample(n=class_sizes[majority_class])\n",
    "    undersampled_majority_labels = y_imbalanced[undersampled_majority_class.index]\n",
    "\n",
    "    minority_class = X_imbalanced[y_imbalanced == minority_class]\n",
    "    minority_class_labels = y_imbalanced[y_imbalanced == minority_class]\n",
    "    \n",
    "    X = pd.concat([undersampled_majority_class, minority_class])\n",
    "    y = pd.concat([undersampled_majority_labels, minority_class_labels])\n",
    "    \n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    reshuffled_index = np.random.permutation(X.index)\n",
    "    X = X.reindex(reshuffled_index)\n",
    "    y = y.reindex(reshuffled_index)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23f9da67-d0df-445a-aefd-275ade52f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Tomek Links\n",
    "# https://towardsdatascience.com/imbalanced-classification-in-python-smote-tomek-links-method-6e48dfe69bbc\n",
    "def tomek_links_auto(X_imbalanced, y_imbalanced):\n",
    "    tl = TomekLinks()\n",
    "    X, y = tl.fit_resample(X_imbalanced, y_imbalanced)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5619b9-02b2-45af-887c-816f9a5adb16",
   "metadata": {},
   "source": [
    "##### Approach 3: Loss function class weighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a3c8f4e-b2e7-47fc-9fae-b2b1e4b7c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Balance the penalties for miscalissification based on class sizes\n",
    "class_weight = 'balanced'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123fdb0-27ae-4afa-8a14-a91dfe7efa43",
   "metadata": {},
   "source": [
    "##### Choosing an approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8eee02c-0cc6-468b-be70-3554d3205936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(X_imbalanced, y_imbalanced):\n",
    "    return duplicate_minority_class(X_imbalanced, y_imbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ba0a4-a9aa-43bc-b301-4f0fbcfe3e02",
   "metadata": {},
   "source": [
    "### Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "976c58bb-5eb5-4411-b4b7-24b9e76c30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_model(X_train, y_train, X_test, y_test, model):\n",
    "    # Impute and scale data\n",
    "    X_train, imputer = simple_imputer(X_train)\n",
    "    X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "    X_train, scaler = simple_scaler(X_train)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Report metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(accuracy)\n",
    "    print(conf_matrix)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "853ac154-29f0-4f52-b664-12636241fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "X = imbalanced_X\n",
    "y = imbalanced_y\n",
    "X_train_imbalanced, X_test, y_train_imbalanced, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "X_train, y_train = balance_classes(X_train_imbalanced, y_train_imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f886724-0306-4133-bfc6-de7474b80f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mtrain_eval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[50], line 9\u001b[0m, in \u001b[0;36mtrain_eval_model\u001b[0;34m(X_train, y_train, X_test, y_test, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scaler\u001b[38;5;241m.\u001b[39mtransform(X_test), columns\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Fit model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Predict test data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1276\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1272\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1273\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1274\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs))\n\u001b[1;32m   1275\u001b[0m         )\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/site-packages/sklearn/svm/_base.py:1215\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1212\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m   1214\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m-> 1215\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mliblinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m n_iter_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(penalty='l2', class_weight=class_weight, solver='liblinear', random_state=random_state),\n",
    "    'Decision Tree': DecisionTreeClassifier(class_weight=class_weight, random_state=random_state),\n",
    "    'Random Forest': RandomForestClassifier(class_weight=class_weight, random_state=random_state),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=random_state),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=[100, 100], random_state=random_state)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}\")\n",
    "    train_eval_model(X_train, y_train, X_test, y_test, model)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
